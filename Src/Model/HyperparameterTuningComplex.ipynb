{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING - COMPLEX MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import modules.preprocessing.sampling as sampling\n",
    "import modules.preprocessing.scaling as scaling\n",
    "import modules.constants as const\n",
    "\n",
    "import numpy as np\n",
    "import modules.training.LSTMmodels as LSTMmodels\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import modules.training.training as training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "\n",
    "import modules.utils as utils\n",
    "import modules.plot_utils as plutils\n",
    "import modules.plot_constants as pltconst\n",
    "from modules.plot_constants import uzh_colors\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "market_df = pd.read_csv(const.input_X_dir + \"Market.csv\")\n",
    "network_df = pd.read_csv(const.input_X_dir + \"NetworkActivity.csv\")\n",
    "social_df = pd.read_csv(const.input_X_dir + \"SocialNetworks.csv\")\n",
    "supply_df = pd.read_csv(const.input_X_dir + \"Supply.csv\")\n",
    "technical_df = pd.read_csv(const.input_X_dir + \"TechnicalIndicators.csv\")\n",
    "\n",
    "# Target feature and dates\n",
    "df_y = pd.read_csv(const.input_y_dir + \"Target.csv\")\n",
    "dates_df = pd.read_csv(const.input_y_dir + \"Dates.csv\")\n",
    "\n",
    "y = df_y.values\n",
    "dates = dates_df.values.flatten()\n",
    "\n",
    "figures_dir = const.tezos_results_dir + \"Hyperparameter tuning/Figures/\"\n",
    "tables_dir = const.tezos_results_dir + \"Hyperparameter tuning/Tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltconst.set_plot_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION_SPLITS = 6\n",
    "N_ATTENTION_HEADS = 8\n",
    "LR = 0.001\n",
    "\n",
    "n_hidden_options1 = [128, 256]\n",
    "n_hidden_options2 = [32, 64, 128]\n",
    "n_epochs = 100\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of hyperparameter tuning, I will create a validation set from the training data set\n",
    "X = market_df.values\n",
    "\n",
    "n_features = X.shape[1]\n",
    "X_train, y_train, X_test, y_test, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=WINDOW_SIZE, step_size=STEP_SIZE, do_segmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models and measure performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [10:44, 107.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [10:59, 109.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [11:41, 116.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [11:03, 110.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:07:16, 672.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [13:27, 134.66s/it]\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "dest_file = tables_dir + \"Complex/Complex_neurons2.csv\"\n",
    "\n",
    "# Iterate through the splits and perform training/testing\n",
    "results = []\n",
    "\n",
    "file_exists = os.path.exists(dest_file)\n",
    "if file_exists: \n",
    "    prev_results = pd.read_csv(dest_file)\n",
    "\n",
    "\n",
    "for n_hidden1 in n_hidden_options1:\n",
    "    for n_hidden2 in n_hidden_options2:\n",
    "        print(n_hidden1, n_hidden2)\n",
    "        if file_exists:\n",
    "            tgt = prev_results[(prev_results.hidden_neurons_layer1 == n_hidden1) & (prev_results.hidden_neurons_layer2 == n_hidden2)]\n",
    "            if tgt.shape[0] > 0: continue\n",
    "\n",
    "        result_row = { \"hidden_neurons_layer1\": n_hidden1, \"hidden_neurons_layer2\": n_hidden2}\n",
    "        tr_loss, val_loss = [], []\n",
    "        training_curves, validation_curves = [], []\n",
    "        \n",
    "        # Iterate over blocked validation splits\n",
    "        for train_indexes, val_indexes in tqdm(tscv.split(X_train)):\n",
    "            X_tr, y_tr, X_val, y_val = X_train[train_indexes], y_train[train_indexes], X_train[val_indexes], y_train[val_indexes]\n",
    "\n",
    "            model = LSTMmodels.LSTMMultiLayerWithAttention(input_dim=X_tr.shape[2], hidden_dim1 = n_hidden1, hidden_dim2=n_hidden2, num_heads=N_ATTENTION_HEADS, output_dim=OUTPUT_DIM)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "            data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=BATCH_SIZE)\n",
    "\n",
    "            # for xtrain, ytrain in data_loader:\n",
    "            #     print(xtrain.shape, ytrain.shape)\n",
    "            model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=n_epochs, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "\n",
    "            training_curves.append(train_loss_curve)\n",
    "            validation_curves.append(validation_loss_curve)\n",
    "            predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "            tr_loss.append(train_loss_curve[-1])\n",
    "            val_loss.append(val_loss_value)\n",
    "\n",
    "        result_row[\"train_loss\"]= np.average(tr_loss)\n",
    "        result_row[\"validation_loss\"] = np.average(val_loss)\n",
    "\n",
    "        result_row[\"training_curve\"] = np.average(np.array(training_curves), axis = 0)\n",
    "        result_row[\"validation_curve\"] = np.average(np.array(validation_curves), axis = 0)\n",
    "\n",
    "        results.append(result_row)\n",
    "\n",
    "        if len(results) == 0: continue\n",
    "        df1 = pd.DataFrame.from_dict(results).sort_values(by=\"validation_loss\", ascending=True).reset_index(drop=True)\n",
    "        if file_exists:\n",
    "            df1 = pd.concat([prev_results, df1], axis= 0)\n",
    "        df1.to_csv(dest_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen configuration 1. : Parameters and Plots\n",
    "\n",
    "Below is the configuration of the chosen model, based on the lowest validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "CHOSEN_N_EPOCHS = 100\n",
    "CHOSEN_N_HIDDEN = 128\n",
    "CHOSEN_LR = 0.001\n",
    "\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Model Num Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION_SPLITS = 4\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "N_HIDDEN1 = 128\n",
    "N_HIDDEN2 = 32\n",
    "N_EPOCHS = 100\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "attention_head_options= [4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of hyperparameter tuning, I will create a validation set from the training data set\n",
    "X = market_df.values\n",
    "\n",
    "n_features = X.shape[1]\n",
    "X_train, y_train, X_test, y_test, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=WINDOW_SIZE, step_size=STEP_SIZE, do_segmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [1:43:48, 6228.54s/it]"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "dest_file = tables_dir + \"Complex/Complex_num_heads.csv\"\n",
    "\n",
    "# Iterate through the splits and perform training/testing\n",
    "results = []\n",
    "\n",
    "file_exists = os.path.exists(dest_file)\n",
    "if file_exists: \n",
    "    prev_results = pd.read_csv(dest_file)\n",
    "\n",
    "\n",
    "for num_heads in attention_head_options:\n",
    "        if file_exists:\n",
    "            tgt = prev_results[(prev_results.num_heads == num_heads)]\n",
    "            if tgt.shape[0] > 0: continue\n",
    "\n",
    "        result_row = { \"num_heads\": num_heads }\n",
    "        tr_loss, val_loss = [], []\n",
    "        training_curves, validation_curves = [], []\n",
    "        \n",
    "        # Iterate over blocked validation splits\n",
    "        for train_indexes, val_indexes in tqdm(tscv.split(X_train)):\n",
    "            X_tr, y_tr, X_val, y_val = X_train[train_indexes], y_train[train_indexes], X_train[val_indexes], y_train[val_indexes]\n",
    "\n",
    "            model = LSTMmodels.LSTMMultiLayerWithAttention(input_dim=X_tr.shape[2], hidden_dim1 = N_HIDDEN1, hidden_dim2=N_HIDDEN2, num_heads=num_heads, output_dim=OUTPUT_DIM)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "            data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=BATCH_SIZE)\n",
    "\n",
    "            model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=N_EPOCHS, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "\n",
    "            training_curves.append(train_loss_curve)\n",
    "            validation_curves.append(validation_loss_curve)\n",
    "            predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "            tr_loss.append(train_loss_curve[-1])\n",
    "            val_loss.append(val_loss_value)\n",
    "\n",
    "        result_row[\"train_loss\"]= np.average(tr_loss)\n",
    "        result_row[\"validation_loss\"] = np.average(val_loss)\n",
    "\n",
    "        result_row[\"training_curve\"] = np.average(np.array(training_curves), axis = 0)\n",
    "        result_row[\"validation_curve\"] = np.average(np.array(validation_curves), axis = 0)\n",
    "\n",
    "        results.append(result_row)\n",
    "\n",
    "        if len(results) == 0: continue\n",
    "        df1 = pd.DataFrame.from_dict(results).sort_values(by=\"validation_loss\", ascending=True).reset_index(drop=True)\n",
    "        if file_exists:\n",
    "            df1 = pd.concat([prev_results, df1], axis= 0)\n",
    "        df1.to_csv(dest_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
