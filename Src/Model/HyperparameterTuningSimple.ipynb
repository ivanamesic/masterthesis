{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING SIMPLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import modules.preprocessing.sampling as sampling\n",
    "import modules.preprocessing.scaling as scaling\n",
    "import modules.constants as const\n",
    "\n",
    "import numpy as np\n",
    "import modules.training.LSTMmodels as LSTMmodels\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import modules.training.training as training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "\n",
    "import modules.utils as utils\n",
    "import modules.plot_utils as plutils\n",
    "import modules.plot_constants as pltconst\n",
    "from modules.plot_constants import uzh_colors\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "market_df = pd.read_csv(const.input_X_dir + \"Market.csv\")\n",
    "network_df = pd.read_csv(const.input_X_dir + \"NetworkActivity.csv\")\n",
    "social_df = pd.read_csv(const.input_X_dir + \"SocialNetworks.csv\")\n",
    "supply_df = pd.read_csv(const.input_X_dir + \"Supply.csv\")\n",
    "technical_df = pd.read_csv(const.input_X_dir + \"TechnicalIndicators.csv\")\n",
    "\n",
    "# Target feature and dates\n",
    "df_y = pd.read_csv(const.input_y_dir + \"Target.csv\")\n",
    "dates_df = pd.read_csv(const.input_y_dir + \"Dates.csv\")\n",
    "\n",
    "y = df_y.values\n",
    "dates = dates_df.values.flatten()\n",
    "\n",
    "figures_dir = const.tezos_results_dir + \"Hyperparameter tuning/Figures/\"\n",
    "tables_dir = const.tezos_results_dir + \"Hyperparameter tuning/Tables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ONE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These hyperparameters were chosen and will stay fixed. The output dimension is determined by the nature of the prediction task.\n",
    "# The window and step sizes are chosen as they are often used with similar tasks (such as stock price prediction), since a longer time window can better capture longer trends\n",
    "\n",
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of hyperparameter tuning, I will create a validation set from the training data set\n",
    "X = market_df.values\n",
    "\n",
    "n_features = X.shape[1]\n",
    "X_train, y_train, X_test, y_test, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=WINDOW_SIZE, step_size=STEP_SIZE, do_segmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION_SPLITS = 6\n",
    "\n",
    "n_hidden_options = [32, 64, 128, 256]\n",
    "lr_options = [0.1, 0.01, 0.001]\n",
    "n_epochs = 200\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.1) Hidden neuron numbers and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "\n",
    "# Iterate through the splits and perform training/testing\n",
    "results = {}\n",
    "training_curves_all, validation_curves_all = {}, {}\n",
    "\n",
    "for n_hidden in n_hidden_options:\n",
    "    print(\"Hidden neurons: \", n_hidden)\n",
    "    results[str(n_hidden)] = {}\n",
    "    training_curves_all[str(n_hidden)] = {}\n",
    "    validation_curves_all[str(n_hidden)] = {}\n",
    "\n",
    "    for lr in lr_options:\n",
    "        tr_loss, val_loss = [], []\n",
    "        training_curves, validation_curves = [], []\n",
    "        \n",
    "        # Iterate over blocked validation splits\n",
    "        for train_indexes, val_indexes in tqdm(tscv.split(X_train)):\n",
    "            X_tr, y_tr, X_val, y_val = X_train[train_indexes], y_train[train_indexes], X_train[val_indexes], y_train[val_indexes]\n",
    "\n",
    "            model = LSTMmodels.LSTMSimple(input_size=X_tr.shape[2], hidden_size=n_hidden, output_size=OUTPUT_DIM)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=X_tr.shape[0])\n",
    "            model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=200, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "\n",
    "            training_curves.append(train_loss_curve)\n",
    "            validation_curves.append(validation_loss_curve)\n",
    "            predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "            tr_loss.append(train_loss_curve[-1])\n",
    "            val_loss.append(val_loss_value)\n",
    "\n",
    "        results[str(n_hidden)][str(lr)] = (np.average(tr_loss), np.average(val_loss))\n",
    "        training_curves_all[str(n_hidden)][str(lr)] = np.average(np.array(training_curves), axis = 0)\n",
    "        validation_curves_all[str(n_hidden)][str(lr)] = np.average(np.array(validation_curves), axis = 0)\n",
    "        \n",
    "df1 = pd.DataFrame.from_dict(results).reset_index()\n",
    "df2 = pd.DataFrame.from_dict(training_curves_all).reset_index() \n",
    "df3 = pd.DataFrame.from_dict(validation_curves_all).reset_index() \n",
    "\n",
    "df1.to_csv(const.data_dir + \"Temp/Results_of_benchmark_tuning1.csv\", index=False)\n",
    "df2.to_csv(const.data_dir + \"Temp/Training_curves_benchmark1.csv\", index=False)\n",
    "df3.to_csv(const.data_dir + \"Temp/Validation_curves_benchmark1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen configuration 1. : Parameters and Plots\n",
    "\n",
    "Below is the configuration of the chosen model, based on the lowest validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_N_EPOCHS = 100\n",
    "CHOSEN_N_HIDDEN = 128\n",
    "CHOSEN_LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.2) Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = market_df.values\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "\n",
    "n_features = X.shape[1]\n",
    "X_train, y_train, X_test, y_test, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=WINDOW_SIZE, step_size=STEP_SIZE, do_segmentation=False)\n",
    "\n",
    "OUTPUT_DIM = 1\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "CHOSEN_N_EPOCHS = 100\n",
    "CHOSEN_N_HIDDEN = 128\n",
    "CHOSEN_LR = 0.001\n",
    "\n",
    "N_VALIDATION_SPLITS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256]\n",
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "\n",
    "\n",
    "results = []\n",
    "for batch_size in batch_sizes:\n",
    "    tr_loss, val_loss = [], []\n",
    "    training_curves, validation_curves = [], []\n",
    "    \n",
    "    # Iterate over blocked validation splits\n",
    "    for train_indexes, val_indexes in tqdm(tscv.split(X_train)):\n",
    "        X_tr, y_tr, X_val, y_val = X_train[train_indexes], y_train[train_indexes], X_train[val_indexes], y_train[val_indexes]\n",
    "\n",
    "        model = LSTMmodels.LSTMSimple(input_size=X_tr.shape[2], hidden_size=CHOSEN_N_HIDDEN, output_size=OUTPUT_DIM)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CHOSEN_LR)\n",
    "        data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=batch_size)\n",
    "        model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=CHOSEN_N_EPOCHS, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "        plt.plot(range(len(validation_loss_curve)), validation_loss_curve)\n",
    "\n",
    "        training_curves.append(train_loss_curve)\n",
    "        validation_curves.append(validation_loss_curve)\n",
    "        predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "        tr_loss.append(train_loss_curve[-1])\n",
    "        val_loss.append(val_loss_value)\n",
    "\n",
    "    plt.show()\n",
    "    results.append({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"train_loss\": np.average(tr_loss),\n",
    "        \"validation_loss\": np.average(val_loss),\n",
    "        \"training_curve\": np.average(np.array(training_curves), axis = 0),\n",
    "        \"validation_curve\": np.average(np.array(validation_curves), axis = 0)\n",
    "    })\n",
    "\n",
    "plt.show()\n",
    "batches_df = pd.DataFrame.from_dict(results)\n",
    "batches_df.to_csv(tables_dir + \"Batch_size_variations2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.3) Window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = market_df.values\n",
    "\n",
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1\n",
    "N_VALIDATION_SPLITS = 6\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "CHOSEN_N_EPOCHS = 100\n",
    "CHOSEN_N_HIDDEN = 256\n",
    "CHOSEN_LR = 0.001\n",
    "\n",
    "CHOSEN_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [07:29, 74.93s/it]\n",
      "6it [07:19, 73.32s/it]\n",
      "6it [08:31, 85.22s/it] \n",
      "6it [08:59, 89.91s/it] \n"
     ]
    }
   ],
   "source": [
    "window_sizes = [7, 14, 30, 45]\n",
    "dest_file = tables_dir + \"Window_size_variations.csv\"\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "\n",
    "results = []\n",
    "for window_size in window_sizes:\n",
    "    X_train, y_train, X_test, y_test, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=window_size, step_size=STEP_SIZE, do_segmentation=False)\n",
    "    tr_loss, val_loss = [], []\n",
    "    training_curves, validation_curves = [], []\n",
    "    \n",
    "    # Iterate over blocked validation splits\n",
    "    for train_indexes, val_indexes in tqdm(tscv.split(X_train)):\n",
    "        X_tr, y_tr, X_val, y_val = X_train[train_indexes], y_train[train_indexes], X_train[val_indexes], y_train[val_indexes]\n",
    "        model = LSTMmodels.LSTMSimple(input_size=X_tr.shape[2], hidden_size=CHOSEN_N_HIDDEN, output_size=OUTPUT_DIM)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CHOSEN_LR)\n",
    "\n",
    "        data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=CHOSEN_BATCH_SIZE)\n",
    "        model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=CHOSEN_N_EPOCHS, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "\n",
    "        training_curves.append(train_loss_curve)\n",
    "        validation_curves.append(validation_loss_curve)\n",
    "        predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "        tr_loss.append(train_loss_curve[-1])\n",
    "        val_loss.append(val_loss_value)\n",
    "\n",
    "    results.append({\n",
    "        \"window_size\": window_size,\n",
    "        \"train_loss\": np.average(tr_loss),\n",
    "        \"validation_loss\": np.average(val_loss),\n",
    "        \"training_curve\": np.average(np.array(training_curves), axis = 0),\n",
    "        \"validation_curve\": np.average(np.array(validation_curves), axis = 0)\n",
    "    })\n",
    "  \n",
    "windows_df = pd.DataFrame.from_dict(results)\n",
    "windows_df.to_csv(dest_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MULTI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = sampling.calculate_test_size_from_date(const.test_start_date)\n",
    "WINDOW_SIZE = 30\n",
    "STEP_SIZE = 1\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION_SPLITS = 6\n",
    "\n",
    "n_hidden_options = [64, 128, 256, 512]\n",
    "lr_options = [0.01, 0.001]\n",
    "n_epochs = 150\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# Now the input parameters are coming from different data groups. For this purpose, data was joined from 3 input groups\n",
    "X = utils.merge_dataframes([market_df, network_df, supply_df])\n",
    "\n",
    "n_features = X.shape[1]\n",
    "X_train2, y_train2, X_test2, y_test2, scaler = sampling.prepare_input_data(X, y, test_size=TEST_SIZE, window_size=WINDOW_SIZE, step_size=STEP_SIZE, do_segmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.1) Hidden neurons and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden neurons:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:26, 44.50s/it]\n",
      "6it [04:18, 43.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden neurons:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:16, 42.75s/it]\n",
      "6it [04:32, 45.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden neurons:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [05:32, 55.49s/it]\n",
      "6it [06:28, 64.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden neurons:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [09:29, 94.98s/it] \n",
      "6it [08:36, 86.15s/it]\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_VALIDATION_SPLITS)\n",
    "\n",
    "# Iterate through the splits and perform training/testing\n",
    "results = []\n",
    "\n",
    "for n_hidden in n_hidden_options:\n",
    "    print(\"Hidden neurons: \", n_hidden)\n",
    "\n",
    "    for lr in lr_options:\n",
    "        result_row = {\"learning_rate\": lr, \"hidden_neurons\": n_hidden}\n",
    "        tr_loss, val_loss = [], []\n",
    "        training_curves, validation_curves = [], []\n",
    "        \n",
    "        # Iterate over blocked validation splits\n",
    "        for train_indexes, val_indexes in tqdm(tscv.split(X_train2)):\n",
    "            X_tr, y_tr, X_val, y_val = X_train2[train_indexes], y_train2[train_indexes], X_train2[val_indexes], y_train2[val_indexes]\n",
    "\n",
    "            model = LSTMmodels.LSTMSimple(input_size=X_tr.shape[2], hidden_size=n_hidden, output_size=OUTPUT_DIM)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            data_loader = sampling.make_data_loader(X_tr, y_tr, batch_size=X_tr.shape[0])\n",
    "            model, train_loss_curve, validation_loss_curve = training.train_model(model, data_loader, n_epochs=n_epochs, optimizer=optimizer, loss_fn = mse_loss, X_val=X_val, y_val=y_val)\n",
    "\n",
    "            training_curves.append(train_loss_curve)\n",
    "            validation_curves.append(validation_loss_curve)\n",
    "            predictions, val_loss_value = training.make_prediction(model, X_val, y_val, mse_loss)\n",
    "\n",
    "            tr_loss.append(train_loss_curve[-1])\n",
    "            val_loss.append(val_loss_value)\n",
    "\n",
    "        result_row[\"train_loss\"]= np.average(tr_loss)\n",
    "        result_row[\"validation_loss\"] = np.average(val_loss)\n",
    "\n",
    "        result_row[\"training_curve\"] = np.average(np.array(training_curves), axis = 0)\n",
    "        result_row[\"validation_curve\"] = np.average(np.array(validation_curves), axis = 0)\n",
    "\n",
    "        results.append(result_row)\n",
    "        \n",
    "df1 = pd.DataFrame.from_dict(results).sort_values(by=\"validation_loss\", ascending=True).reset_index()\n",
    "df1.to_csv(tables_dir + \"Results_of_benchmark_tuning2.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
